---
title: 'On Bayes theorem and Bayesian inference'
date: 2025-10-4
permalink: /posts/2025/10/blog-post-1/
tags:
  - Bayesian
---

<script>
  window.MathJax = {
    tex: {
      tags: 'all',
      packages: {'[+]': ['ams']}
    }
  };
</script>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

# Bayes' Theorem 

The way that I understand the Bayes theorem is as follows. Let' say we want to calculate the joint probablity of $$p(x,y)$$, meaning the probability of having both $$x$$ and $$y$$ simultaneously. These are two tochastic variables that somehow are related or could be related to each other. For example, $$x$$ shows whether it is raining or not, and $$y$$ could show whether your neighbor picks up their umberella. Note that the adverb *simultaneously* does not mean they should definitely occur at the same *time*; their *actual* time of occurance could be different. *Simultaneously* here simply means logical AND operator. For instance, one could ask about the joint probability of attending a class during a term and passing its exam which comes in the end. 

To undestand the theorem I usually go back to the frequentistic approach of probability and consider an example. It is handy to have an example at hand and understand all in that context. Let's consider the following example:

There is a city with two areas $(E, W)$. Each household has one car. Cars could have 3 colors $$(R, G, B)$$. Let's say there are 100 housholds, 40 of which in area $$E$$ and 60 of which in area $$W$$. To present the cars' colors we can use a tree structure:

- E
    - R: 14
    - G: 9
    - B: 17
- W
    - R: 25
    - G: 16
    - B: 19

We can, of course, look at it in a different way. We can start the tree by first dividing the housholds by their cars' color and then by the area they live in:

- R
    - E: 14
    - W: 25
- G
    - E: 9
    - W: 16
- B
    - E: 17
    - W: 19

We can now ask, how many people live in area $$E$$ *AND* (the logical one, *simultaneously*) have a blue car? To answer that we can do it in two ways:
1. Either first select people who live in area $E$, which are 40. And then see that among those 17 households have a blue car.
2. Or first select households with blue cars which are 36, and then select those who live in area *E* among them. Also lead to 17! 

> So the order we do the counting/selection does not matter.

Now let's go back to the joint probability. Let's ask what is the probability of randomly choosing a houshold in area $E$ with a blue car? 

Well, the result is $$17%$$, because 17 out of 100 household fulfill both conditions. 

$$
\begin{equation}
p(\textrm{area} = E, \textrm{color} = B) = \frac{17}{100} \ . 
\end{equation}
$$

Let's calculate it using probabilities. Again we can do it in two ways:

We can first ask ourselves what is the probability of choosing a household from the area $$E$$. It is 40 out of 100: 

$$
\begin{aligned}
p(\textrm{area} = E) &= \frac{40}{100} \\
        &= 0.4 \ .
\end{aligned}
$$

We can now ask that what is the probability of having a blue cae a houshold in region $$E$$? Or in a bit more technical term, *given* a household in region $$E$$, what is such a probability? 

$$
\begin{equation}
p(\textrm{color} = B | \textrm{area} = E) = \frac{17}{40} \ .
\label{p_B_given_E}
\end{equation}
$$

As seen in Eq. \(\ref{p_B_given_E}\), probability of having a blue car **among those in area E** is $$17/40$$. Ok We can now simply use the product rule, or essentially logic. We know $$40\%$$ live in area E, **of whcih** $$42.5\% (17/40)$$ have a blue car. So the result must be $$ 0.4 \times 0.425 = 0.17$$. This way of thinking corresponds to traversing the first tree. So we can write:

$$
\begin{equation}
p(\textrm{area} = E, \textrm{color} = B) = p(\textrm{color} = B | \textrm{area} = E) p(\textrm{area} = E) \ .
\end{equation}
$$

In the second way, we can traverse the second tree. As we can see the probability of having a blue car is $$36% (36/100$$. The probability of **living in area E amon the people with a blue car**  is $$47.2\%(17/36)$$.So $$ 36\% $$ have a blue car, **of whcih** $$47.2\% $$ live in area E. So the result must be $$ 0.36 \times 0.47222 = 0.17$$. The same as it should be! For this way of thinking we can write:

$$
\begin{equation}
p(\textrm{area} = E, \textrm{color} = B)= p(\textrm{area} = E |\textrm{color} = B) p(\textrm{color} = B) \ .
\end{equation}
$$

And as we discussed both ways should give us the same result and they do. 

$$
\begin{aligned}
p(\textrm{area} = E, \textrm{color} = B) &= p(\textrm{color} = B | \textrm{area} = E) p(\textrm{area} = E) \\  
                & =  p(\textrm{area} = E|\textrm{color} = B) p(\textrm{color} = B) \ .
\end{aligned}
$$

This was an example. We can now write this for any two random variables as: 

$$
\begin{equation}
p(x,y) = p(x|y) p(y) = p(y|x) p(x) \ . 
\label{p_2_variables}
\end{equation}
$$

And what is usually called the **Bayes rule** is derived from the Eq. \(\ref{p_2_variables}\): 

$$
\begin{equation}
p(x|y) =  \frac{ p(y|x) p(x) }{p(y)}  \ . 
\label{Bayes}
\end{equation}
$$

There are some naming convention which twould make more sense as we look at a few examples below:
- $$ p(x) $$: **Prior** or **Initial Belief**,
- $$ p(y\|x) $$: **Likelihood**,
- $$ p(x\verty) $$: **Posterior** / **Updaed Belief**,
- $$ p(y) $$: **Marginal likelihood**.

Some remarks:
- $$ p(x) $$ is a probability distribution, in the sense that it is summed to integerated to 1 ($$ \sum_{x} p(x) = 1$$). The same for $$ p(y)$$ . 
- $$ p(y|x) $$ **is** a probabilty distribution **over** $$y$$. This is the probability of having $$y$$ **given** some information about $$x$$. Because $$y$$ must take a value anyway and given the new information the corresponding distribution should still sum up to 1. But $$ p(y|x) $$  is **not** a distribution over $$x$$. The same story for $$ p(x|y) $$ .
- The reason for the name of $$ p(x) $$ and $$ p(x|y) $$ as prior and posterior or initial and updated will become clear later on. But the genral mindset is that, you have a belief or intital knowledge, hence $$p(x)$$, and after getting more info, or doing experiments or collecting data you **update** your belief about $$ x $$ and reach $$ p(x|y) $$.
- Usually $$ p(y) $$ does not get that much attention, since there is no need for it. Sometimes it is also calledthe *normalization* factor. You could say that the denominatior in the right hand side of Eq. \(\ref{Bayes}\) is the sum of its nominator: $$ p(y) = \sum_{x} p(y|x) p(x)  $$ by the rules of statistic or logic or common sense. And hence the name normaliztion. You could also reach the same conclusion by summing both sides over $$x$$: $$ \sum_x p(x|y) = 1$$ and hence  we have  $$ \sum_x \frac{ p(y|x) p(x) }{p(y)} = \frac{ 1}{p(y)} \sum_x \frac p(y|x) p(x) = 1 $$ which is the same as what we mentioned earlier. 

These names and how we should use the Bayes rule as written in Eq. \(\ref{Bayes}\) will become clear in the following examples. 

# Examples

## Raining - Wet road

## test and infected

## Inference a parameter. 







